# RAG.py
import os
import openai
import jieba
import jieba.posseg as pseg
import uuid
import asyncio
import nest_asyncio
nest_asyncio.apply()

from typing import Dict, List, Any, Optional
from opencc import OpenCC
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


# ç¼“å­˜
from redis import Redis
import pickle
import hashlib

# ragas imports
from ragas import evaluate
from ragas.metrics import (
    faithfulness, 
    answer_relevancy, 
    context_precision,
    context_utilization
)
from ragas.metrics.critique import SUPPORTED_ASPECTS, harmfulness
from ragas.run_config import RunConfig
from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings

# ragas langchain wrappers
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

# langfuse
from langfuse.callback import CallbackHandler
from langfuse.decorators import observe, langfuse_context
from dataclasses import dataclass
from langfuse import Langfuse
from contextlib import contextmanager

@dataclass
class EvalConfig:
    """è¯„ä¼°é…ç½®ç±»"""
    enable: bool = False
    trace_name: str = "historical_qa_eval"
    user_id: str = "default_user"
    metrics: List = None

class HistoricalQA:
    def __init__(
        self, 
        graph, 
        model_name="gpt-4-1106-preview", 
        openai_api_key=None,
        langfuse_public_key=None,
        langfuse_secret_key=None,
        eval_config: Optional[EvalConfig] = None
    ):
        """
        åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ
        Args:
            graph: ä»KnowledgeGraphCreatorä¼ å…¥çš„Neo4jå›¾å®ä¾‹
            model_name: ä½¿ç”¨çš„GPTæ¨¡å‹åç§°
            openai_api_key: OpenAI APIå¯†é’¥
            langfuse_public_key: Langfuseå…¬é’¥
            langfuse_secret_key: Langfuseç§é’¥
            eval_config: è¯„ä¼°é…ç½®
        """
        self.graph = graph
        
        # è®¾ç½®OpenAI APIå¯†é’¥
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        elif "OPENAI_API_KEY" not in os.environ:
            raise ValueError("è¯·æä¾›OpenAI APIå¯†é’¥!")
        
        # è®¾ç½®Langfuseå¯†é’¥
        if langfuse_public_key and langfuse_secret_key:
            os.environ["LANGFUSE_PUBLIC_KEY"] = langfuse_public_key
            os.environ["LANGFUSE_SECRET_KEY"] = langfuse_secret_key
        elif eval_config and eval_config.enable:
            if "LANGFUSE_PUBLIC_KEY" not in os.environ or "LANGFUSE_SECRET_KEY" not in os.environ:
                raise ValueError("è¯„ä¼°æ¨¡å¼éœ€è¦æä¾›Langfuseçš„å…¬é’¥å’Œç§é’¥!")
        
        self._init_custom_dictionary()
        
        # åˆå§‹åŒ–LLMå’ŒEmbeddingæ¨¡å‹
        self.llm = ChatOpenAI(
            model_name=model_name, 
            temperature=0,
            openai_api_key=openai_api_key
        )
        self.embedding_model = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=openai_api_key
        )
        
        # åˆå§‹åŒ–è½¬æ¢å™¨ï¼ˆç®€ä½“åˆ°ç¹ä½“ï¼‰
        self.cc = OpenCC('s2t')
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä½ä¸“ç²¾äºä¸­å›½å¤ä»£å²çš„å†å²å­¦å®¶ï¼Œè¯·åŸºäºæä¾›çš„å²æ–™ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

        å²æ–™ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        å›ç­”è¦æ±‚ï¼š
        1. ä»¥"æ ¹æ®å²æ–™è®°è½½"ã€"æ®å²ä¹¦è®°è½½"ã€"å²æ–™æ˜¾ç¤º"ç­‰ä¸“ä¸šç”¨è¯­å¼€å¤´
        2. æŒ‰ç…§æ—¶é—´é¡ºåºæˆ–é€»è¾‘å…³ç³»ç»„ç»‡ä¿¡æ¯
        3. å¦‚æœæ¶‰åŠå®˜èŒï¼Œéœ€è¦è¯´æ˜å…·ä½“çš„å®˜èŒåç§°
        4. å¦‚æœæ¶‰åŠäººç‰©å…³ç³»ï¼Œéœ€è¦æ˜ç¡®è¯´æ˜å…³ç³»ç±»å‹ï¼ˆå¦‚çˆ¶å­ã€å›è‡£ã€åŒåƒšç­‰ï¼‰
        5. å¦‚æœå²æ–™ä¸­æœ‰å…·ä½“çš„å†å²èƒŒæ™¯æˆ–äº‹ä»¶ï¼Œä¹Ÿè¯·ç®€è¦è¯´æ˜
        6. å¦‚æœä¿¡æ¯ä¸å®Œæ•´æˆ–å­˜åœ¨æ­§ä¹‰ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
        7. ä½¿ç”¨å‡†ç¡®çš„å†å²æœ¯è¯­å’Œå…¸æ•…
        8. å›ç­”è¦ç®€æ˜æ‰¼è¦ï¼Œçªå‡ºé‡ç‚¹
        """)

        # è¯„ä¼°ç›¸å…³åˆå§‹åŒ–
        self.eval_config = eval_config or EvalConfig()
        if self.eval_config.enable:
            self._init_evaluation()
        
        # åˆå§‹åŒ– Langfuse å®¢æˆ·ç«¯
        if eval_config and eval_config.enable:
            self.langfuse = Langfuse(
                public_key=langfuse_public_key,
                secret_key=langfuse_secret_key
            )
        
        try:
            self.redis_client = Redis(
                host='localhost',
                port=6379,
                db=0,
                decode_responses=False
            )
            self.redis_client.ping()
            print("Redisç¼“å­˜æœåŠ¡è¿æ¥æˆåŠŸ")
            self.cache_ttl = 3600  # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        except Exception as e:
            print(f"Redisè¿æ¥å¤±è´¥: {e}")
            print("ç³»ç»Ÿå°†åœ¨æ— ç¼“å­˜æ¨¡å¼ä¸‹è¿è¡Œ")
            self.redis_client = None

    def _init_custom_dictionary(self):
        """åˆå§‹åŒ–è‡ªå®šä¹‰è¯å…¸"""
        query = """
        MATCH (n)
        RETURN DISTINCT n.name as name, labels(n) as labels
        """
        results = self.graph.query(query)
        
        for result in results:
            if result['name']:
                if 'äººç‰©' in result['labels']:
                    jieba.add_word(result['name'], freq=1000, tag='nr')
                elif 'åœ°ç‚¹' in result['labels']:
                    jieba.add_word(result['name'], freq=1000, tag='ns')
                elif 'å®˜è¡”' in result['labels']:
                    jieba.add_word(result['name'], freq=1000, tag='nz')
                else:
                    jieba.add_word(result['name'], freq=1000)
        
        # æ·»åŠ å¸¸ç”¨è¯
        common_words = ['å®˜èŒ', 'çˆ¶æ¯', 'å…„å¼Ÿ', 'ä»»èŒ']
        for word in common_words:
            jieba.add_word(word, freq=1000)

    def _init_evaluation(self):
        """åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ"""
        # è®¾ç½®é»˜è®¤æŒ‡æ ‡
        if not self.eval_config.metrics:
            self.eval_config.metrics = [
                faithfulness, 
                answer_relevancy, 
                harmfulness,
                # context_precision,  # å¯é€‰ï¼Œä½†æ˜¯å¿…é¡»æå‰æä¾›æ­£ç¡®ç­”æ¡ˆ
                # context_utilization  # å¯é€‰ï¼Œä½†æ˜¯å¿…é¡»æå‰æä¾›æ­£ç¡®ç­”æ¡ˆ
            ]
        
        # åˆ›å»ºè¿è¡Œé…ç½®
        run_config = RunConfig()
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        for metric in self.eval_config.metrics:
            if isinstance(metric, MetricWithLLM):
                metric.llm = LangchainLLMWrapper(self.llm)
            if isinstance(metric, MetricWithEmbeddings):
                metric.embeddings = LangchainEmbeddingsWrapper(self.embedding_model)
            if hasattr(metric, 'init'):
                metric.init(run_config)

    async def _score_with_ragas(self, question: str, contexts: List[str], answer: str):
        """å¼‚æ­¥æ‰§è¡Œè¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
        scores = {}
        for metric in self.eval_config.metrics:
            print(f"è®¡ç®—è¯„ä¼°æŒ‡æ ‡: {metric.name}")
            try:
                score_result = metric.ascore({
                    "question": question,
                    "contexts": contexts,
                    "answer": answer
                })
                if asyncio.iscoroutine(score_result):
                    scores[metric.name] = await score_result
                else:
                    scores[metric.name] = score_result
            except Exception as e:
                print(f"è¯„ä¼°æŒ‡æ ‡ {metric.name} è®¡ç®—å¤±è´¥: {str(e)}")
        return scores

    @observe()
    def _get_contexts(self, question: str) -> List[str]:
        """è·å–ç›¸å…³ä¸Šä¸‹æ–‡"""
        names = self._extract_names(question)
        all_results = []
        for name in names:
            results = self._query_graph(name)
            all_results.extend(results)
        
        return [
            f"{r['entity1']}ä¸{r['entity2']}ä¹‹é—´çš„å…³ç³»æ˜¯{r['relation']}ã€‚å…·ä½“æè¿°ï¼š{r['context']}" 
            for r in all_results
        ]

    @observe()
    def _generate_answer(self, chain, question: str, config: Dict) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        response = chain.invoke(
            {
                "input": question,
                "question": question
            },
            config=config
        )
        return response["answer"]

    def _answer_with_evaluation(self, chain, question: str, session_id: Optional[str] = None) -> str:
        """å¸¦è¯„ä¼°çš„é—®ç­”å¤„ç†"""
        # åˆ›å»ºLangfuseå¤„ç†å™¨
        handler = CallbackHandler(
            trace_name=self.eval_config.trace_name,
            user_id=self.eval_config.user_id,
            session_id=session_id or f"session-{str(uuid.uuid4())}"
        )
        
        # é…ç½®å›è°ƒ
        config = {
            "configurable": {
                "session_id": self.eval_config.user_id
            },
            "callbacks": [handler]
        }
        
        # æ‰§è¡Œé—®ç­”
        response = chain.invoke(
            {
                "input": question,
                "question": question
            },
            config=config
        )
        answer = response["answer"]
        
        return answer

    def answer_question(self, question: str, session_id: Optional[str] = None) -> str:
        """å¤„ç†é—®é¢˜å¹¶ç”Ÿæˆç­”æ¡ˆ"""
        names = self._extract_names(question)
        if not names:
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ä»é—®é¢˜ä¸­è¯†åˆ«å‡ºäººåæˆ–åœ°åã€‚"
        
        all_results = []
        for name in names:
            results = self._query_graph(name)
            all_results.extend(results)
            
        if not all_results:
            return f"æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°å…³äº {', '.join(names)} çš„ç›¸å…³å†å²è®°è½½ã€‚"
        
        vector_store = self._create_vector_store(all_results)
        rag_chain = self._create_rag_chain(vector_store)
        
        # æ ¹æ®æ˜¯å¦å¯ç”¨è¯„ä¼°æ¨¡å¼æ¥å¤„ç†
        if self.eval_config.enable:
            return self._answer_with_evaluation(rag_chain, question, session_id)
        else:
            response = rag_chain.invoke({
                "input": question,
                "question": question
            })
            return response["answer"]

    def _extract_names(self, question: str) -> List[str]:
        """æå–é—®é¢˜ä¸­çš„åå­—å¹¶è½¬æ¢ä¸ºç¹ä½“"""
        words = pseg.cut(question)
        names = []
        for word, flag in words:
            if flag in ['nr', 'ns', 'nz', 'n']:
                traditional_word = self.cc.convert(word)
                names.append(traditional_word)
        return names

    def _query_graph(self, name: str) -> List[Dict]:
        """æŸ¥è¯¢å›¾æ•°æ®åº“"""
        query = """
        MATCH (n1 {name: $name})-[r]->(n2)
        RETURN 
            n1.name as entity1,
            type(r) as relation,
            n2.name as entity2,
            r.context as context
        UNION
        MATCH (n1)-[r]->(n2 {name: $name})
        RETURN 
            n2.name as entity1,
            type(r) as relation,
            n1.name as entity2,
            r.context as context
        """
        return self.graph.query(query, {'name': name})

    def _create_vector_store(self, results: List[Dict]) -> FAISS:
        """åˆ›å»ºæˆ–è·å–å‘é‡å­˜å‚¨"""
        if not self.redis_client:
            print("âš ï¸ Redisç¼“å­˜æœªå¯ç”¨ï¼Œå°†ç›´æ¥åˆ›å»ºå‘é‡å­˜å‚¨")
            return self._create_vector_store_without_cache(results)
        
        # ä¸ºæŸ¥è¯¢ç»“æœåˆ›å»ºå”¯ä¸€æ ‡è¯†
        results_str = str(sorted([
            f"{r['entity1']}{r['relation']}{r['entity2']}" 
            for r in results
        ]))
        cache_key = f"vector_store:{hashlib.md5(results_str.encode()).hexdigest()}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = self.redis_client.get(cache_key)
        if cached_data:
            try:
                print("ğŸ¯ æ£€æµ‹åˆ°ç¼“å­˜å‘½ä¸­ï¼æ­£åœ¨ä»RedisåŠ è½½å‘é‡å­˜å‚¨...")
                vector_data = pickle.loads(cached_data)
                embeddings = vector_data['embeddings']
                texts = vector_data['texts']
                return FAISS.from_texts(
                    texts=texts,
                    embedding=self.embedding_model,
                    metadatas=[{"content": text} for text in texts]
                )
            except Exception as e:
                print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        else:
            print("ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨...")
        
        # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
        texts = [
            f"{r['entity1']}ä¸{r['entity2']}ä¹‹é—´çš„å…³ç³»æ˜¯{r['relation']}ã€‚å…·ä½“æè¿°ï¼š{r['context']}" 
            for r in results
        ]
        
        docs = CharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        ).create_documents(texts)
        
        vector_store = FAISS.from_documents(docs, self.embedding_model)
        
        # å­˜å‚¨å‘é‡æ•°æ®è€Œä¸æ˜¯æ•´ä¸ªå‘é‡å­˜å‚¨å¯¹è±¡
        try:
            vector_data = {
                'embeddings': [doc.page_content for doc in docs],  # åªå­˜å‚¨æ–‡æœ¬å†…å®¹
                'texts': [doc.page_content for doc in docs]
            }
            self.redis_client.setex(
                cache_key,
                self.cache_ttl,
                pickle.dumps(vector_data)
            )
            print("âœ… å‘é‡å­˜å‚¨å·²æˆåŠŸç¼“å­˜")
        except Exception as e:
            print(f"âŒ ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
        
        return vector_store

    def _create_vector_store_without_cache(self, results: List[Dict]) -> FAISS:
        """ä¸ä½¿ç”¨ç¼“å­˜åˆ›å»ºå‘é‡å­˜å‚¨"""
        texts = [
            f"{r['entity1']}ä¸{r['entity2']}ä¹‹é—´çš„å…³ç³»æ˜¯{r['relation']}ã€‚å…·ä½“æè¿°ï¼š{r['context']}" 
            for r in results
        ]
        
        docs = CharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        ).create_documents(texts)
        
        return FAISS.from_documents(docs, self.embedding_model)

    def _create_rag_chain(self, vector_store):
        """åˆ›å»ºRAGé“¾"""
        retriever = vector_store.as_retriever(search_kwargs={"k": 100})
        document_chain = create_stuff_documents_chain(
            llm=self.llm,
            prompt=self.prompt,
            document_variable_name="context"
        )
        return create_retrieval_chain(
            retriever=retriever,
            combine_docs_chain=document_chain
        )